#!/usr/bin/env python3
import os
import time
import torch
import requests
import argparse
from io import BytesIO
from transformers import PreTrainedTokenizerFast, LlamaForCausalLM
from pruna_pro import smash, SmashConfig
from hi_diffusers import HiDreamImagePipeline, HiDreamImageTransformer2DModel
from hi_diffusers.schedulers.flash_flow_match import FlashFlowMatchEulerDiscreteScheduler

# ‚Äî Telegram Bot Credentials ‚Äî
BOT_TOKEN = '*********************'
CHAT_ID   = '************'

# ‚Äî Pruna Pro API token ‚Äî
os.environ["PRUNA_TOKEN"] = '*****************'

# ‚Äî Model configuration (dev) ‚Äî
MODEL_PREFIX = "HiDream-ai"
LLAMA_MODEL  = "meta-llama/Meta-Llama-3.1-8B-Instruct"
MODEL_CONFIGS = {
    "dev": {
        "path": f"{MODEL_PREFIX}/HiDream-I1-Dev",
        "guidance_scale": 0.0,
        "num_inference_steps": 28,
        "shift": 6.0,
        "scheduler": FlashFlowMatchEulerDiscreteScheduler
    }
}

def send_telegram(img, caption: str):
    buf = BytesIO()
    img.save(buf, format="PNG")
    buf.seek(0)
    requests.post(
        f"https://api.telegram.org/bot{BOT_TOKEN}/sendPhoto",
        data={"chat_id": CHAT_ID, "caption": caption},
        files={"photo": ("image.png", buf, "image/png")}
    )

def load_pipeline():
    cfg  = MODEL_CONFIGS["dev"]
    path = cfg["path"]
    scheduler = cfg["scheduler"](
        num_train_timesteps=1000,
        shift=cfg["shift"],
        use_dynamic_shifting=False
    )

    tokenizer = PreTrainedTokenizerFast.from_pretrained(
        LLAMA_MODEL, use_fast=True, local_files_only=True
    )
    text_encoder = LlamaForCausalLM.from_pretrained(
        LLAMA_MODEL,
        torch_dtype=torch.bfloat16,
        attn_implementation="eager",
        low_cpu_mem_usage=True,
        use_safetensors=True,
        local_files_only=True
    ).to("cuda", non_blocking=True)
    transformer = HiDreamImageTransformer2DModel.from_pretrained(
        path,
        subfolder="transformer",
        torch_dtype=torch.bfloat16,
        use_safetensors=True,
        local_files_only=True
    ).to("cuda", non_blocking=True)

    pipe = HiDreamImagePipeline.from_pretrained(
        path,
        scheduler=scheduler,
        tokenizer_4=tokenizer,
        text_encoder_4=text_encoder,
        torch_dtype=torch.bfloat16,
        use_safetensors=True,
        local_files_only=True
    ).to("cuda", torch.bfloat16)
    pipe.transformer = transformer
    return pipe, cfg

def parse_resolution(res: str):
    w,h = res.split("√ó")
    return int(w.strip()), int(h.split()[0].strip())

if __name__ == "__main__":
    # performance flags
    os.environ["SAFETENSORS_FAST_GPU"]               = "1"
    torch.backends.cuda.matmul.allow_tf32            = True
    torch.backends.cudnn.allow_tf32                  = True
    torch.backends.cudnn.benchmark                   = True
    torch.backends.cuda.flash_sdp_enabled            = True
    torch.backends.cuda.memory_efficient_sdp_enabled = True
    torch.backends.cuda.math_sdp_enabled             = True
    torch.set_grad_enabled(False)

    parser = argparse.ArgumentParser()
    parser.add_argument("--resolution", type=str, default="1024 √ó 1024 (Square)")
    args = parser.parse_args()

    total_start = time.time()
    print("Loading 'dev' pipeline‚Ä¶")
    t0 = time.time()
    pipe, cfg = load_pipeline()
    load_time = time.time() - t0
    print(f"‚úÖ Loaded in {load_time:.2f}s")

    # Pruna smash config
    smash_cfg = SmashConfig()
    smash_cfg["cacher"]            = "auto"
    smash_cfg["auto_cache_mode"]   = "taylor"
    smash_cfg["auto_speed_factor"] = 0.5
    smash_cfg["auto_objective"]    = "fidelity"

    print("üîß Trying Pruna Pro optimization‚Ä¶")
    try:
        ts = time.time()
        pipe = smash(pipe, smash_cfg, token=os.environ["PRUNA_TOKEN"])
        smash_time = time.time() - ts
        print(f"‚úÖ Pruna smash in {smash_time:.2f}s")
    except Exception as e:
        print(f"‚ö†Ô∏è Pruna smash failed: {e}")
        smash_time = 0.0
        print("‚ö†Ô∏è Skipping compile fallback to avoid dynamo errors.")

    # light pipeline tweaks
    pipe.transformer.to(memory_format=torch.channels_last)
    pipe.vae.to(memory_format=torch.channels_last)
    pipe.scheduler.sigmas = pipe.scheduler.sigmas.to("cpu")

    prompt = "A cat holding a sign that says 'Hi-Dreams.ai'."
    width, height = parse_resolution(args.resolution)
    seed = torch.randint(0, 1_000_000, (1,)).item()

    # warm-up (1 step)
    _ = pipe(
        prompt,
        height=height, width=width,
        num_inference_steps=1,
        guidance_scale=0.0,
        generator=torch.Generator("cuda").manual_seed(seed)
    )
    torch.cuda.synchronize()

    print("Generating image‚Ä¶")
    t1 = time.time()
    out = pipe(
        prompt,
        height=height, width=width,
        num_inference_steps=cfg["num_inference_steps"],
        guidance_scale=cfg["guidance_scale"],
        generator=torch.Generator("cuda").manual_seed(seed)
    )
    torch.cuda.synchronize()
    infer_time = time.time() - t1
    total_time = time.time() - total_start

    print(f"‚úÖ Inference: {infer_time:.2f}s | Total: {total_time:.2f}s")
    caption = (
        f"Load: {load_time:.2f}s\n"
        f"Pruna smash: {smash_time:.2f}s\n"
        f"Infer: {infer_time:.2f}s\n"
        f"Total: {total_time:.2f}s\n"
        f"Seed: {seed}"
    )
    send_telegram(out.images[0], caption)
    print("Done.")
